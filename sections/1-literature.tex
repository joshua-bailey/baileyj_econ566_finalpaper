\pagebreak
\section{Introduction and Literature Review: Machine Learning and Inflation Forecasting} \label{sec:lit}

\subsection{Inflation Forecasting and Machine Learning: A Brief Review} \label{sec:lit_review}

Inflation forecasting has long been a central interest of empirical macroeconomics.\footnote{See, for instance, \textcite{Gordon1990U.S.Unemployment} and \textcite{Stock1999ForecastingInflation}.}. Central banks, financial institutions, large corporates, and households all have a strong interest in accurate, timely forecasts of inflation. At the birth of modern macroeconomics, theorists like \textcite{Tinter1940Business1919-1932} established the foundations of the discipline, before the advent of large Keynesian macroeconomic models with many equations, solved simultaneously, provided the basis of modern macro forecasting \autocite{Ekeblad1952Economic1921-1941.}. These models evovled into the vector autoreggresive (VAR) models that are the workhorse tools of forecasting today \autocite{Sims1982PolicyModels}.

This general approach to empirical macroeconomic forecasting famously came under attack, first by \textcite{Friedman1968ThePolicy}, who argued that the standard Phillips curve relationship \autocite{Phillips1958The186119571} did not represent a static policy trade-off between unemployment and inflation, and that the underlying relationship was much more dynamic and subject to change in the face of shocks. Friedman's critique was formalised by \textcite{Lucas1976EconometricCritique} with a formal model. Lucas ushered in the `rational expectations' revolutions to macroeconomics and brought with a dynamic understanding of the Phillips curve. 

Many view the Lucas critique as a fundamental for macroeconomic forecasting. If underlying relationships have the dynamics described by Lucas, what is the point in modelling the historic relationship $Y \sim X $. The response is two-fold. At one level, Lucas' argument is just a call for models to be more dynamic. It should be obvious that response elasticities are not likely to be constant, while other factors in a model evolve over time. Better empirical models need to be sensitive to changing conditions and complex underlying relationships between variables.  

More fundamentally, challenges in isolating causal factors in econometric modelling does not obviate the need for policymakers to engage in such analysis. Consider the argument made in \textcite{Kleinberg2015PredictionProblems}. In their paper, they set out a toy problem faced by a policymaker:

\begin{quote}
    \textit{``Let $Y$ be an outcome variable (such as rain) which depends in an unknown way on a set of variables $X_0$ and $X$. A policy-maker must decide on $X_0$ (e.g. an umbrella or rain-dance) in order to maximize a (known) payoff function $\pi\left(X_0, Y\right)$. Our decision of $X_0$ depends on the derivative:}

    $$
    \frac{d \pi\left(X_0, Y\right)}{d X_0}=\frac{\partial \pi}{\partial X_0} \underbrace{(Y)}_{\text {prediction }}+\frac{\partial \pi}{\partial Y} \underbrace{\frac{\partial Y}{\partial X_0}}_{\text {causation }}"
    $$
\end{quote}

The policymaker has to solve for both terms, $\frac{\partial Y}{\partial X_0}$ and $\frac{\partial \pi}{\partial X_0}$. To do this, the term, $\frac{\partial \pi}{\partial X_0}$, must be evaluated at $Y$, which needs to be estimated through prediction. In the language of their hypothetical, the rain dance choice is a purely causal question -- because rain dances have no effect on the payoffs -- but umbrellas are a pure prediction problem because umbrellas have no effect on rain. 

In the realm of macroeconomic policymaking, particularly in forecasting inflation, the dichotomy between causality and prediction—akin to the umbrella and rain dance analogy—proves highly instructive. For instance, consider a central bank tasked with maintaining price stability, confronting decisions akin to whether to engage in a rain dance or merely prepare an umbrella. The 'rain dance' in this scenario is analogous to a central bank's decision to adjust interest rates or implement quantitative easing, grounded in the causal belief that such actions will directly influence inflation rates. This causal inference demands a robust understanding of economic dynamics, including how monetary policy adjustments lead to changes in consumer behaviour, investment, and ultimately, price levels. Here, the policy's efficacy hinges on accurately discerning and manipulating the causal levers of the economy.

The 'umbrella' approach is exemplified by predictive tasks, such as using economic indicators to forecast inflation trends. Here, the central bank, much like an individual deciding whether to carry an umbrella based on weather predictions, utilizes models to predict future inflation rates based on current data. This predictive inference does not require understanding or establishing the causality between observed data and future states but focuses on accurately forecasting outcomes to inform policy decisions. Beyond a central bank, financial firms or households have various responses at their disposal if they predict higher inflation, that do not depend on having a causal effect on inflation, e.g., increase savings, cut back on expenditure, and pay down debts. In practice, effective macroeconomic management often necessitates a blend of both approaches. While predictive models, enhanced by machine learning algorithms, provide high-accuracy forecasts to anticipate economic conditions, understanding the causal impact of policy instruments is crucial for crafting interventions that achieve desired economic objectives. This integration of causal and predictive insights enables policymakers to not only react to economic developments but also to strategically influence future economic outcomes, balancing immediate actions with long-term planning.\footnote{This leaves aside the body of machine learning literature that focuses on causal relationships as well. The point being made here is that, \textit{even if} a model is only purely predictive, it is still an important tool for governments, households, and firms.} Central banks themselves have recognised the importance of improving inflation prediction accuracy as a research goal unto itself \autocite{Chakraborty2017MachineBanks,SmalterHall2018MachineForecasting}.


Despite its importance, it has been, as \textcite{Stock2010ModelingCrisis} noted, `` exceedingly difficult to improve systematically upon simple univariate forecasting models, such as the \textcite{Atkeson2001AreInflation} random walk model [...] or the time-varying unobserved components model in \textcite{Stock2007WhyForecast}.” Machine learning offers the potential to substantially improve these efforts. The complex relationships that govern the macroeconomy lend themselves to the techniques offered by machine learning. To take just two advantages, machine learning methods can handle larger datasets and handle the nonlinear relationships inherent in the working of the macroeconomy. This sub-set of the literature also has a long history. \textcite{Stock2023ASeries} recount early efforts of theirs in the late 1990s to look at the use of neural networks to forecast inflation. \textcite{Nakamura2005InflationNetwork} also exploited a simple neural network to forecast inflation, comparing it to a univariate autoregression model and finding that the neural network improved forecasting performance. 

While these early models often showed incremental improvements on existing methods, they were limited to the availability of good data and computational power. As a result of recent advances in these areas, there has been a significant growth in research examining the application of machine learning to a range of areas of economics \autocite{Athey2019TheEconomics}.

In the field of inflation forecasting, \textcite{Kleinberg2015PredictionProblems} explore the benefits and limitations of using ML approaches for economic forecasting. They advocate for ML as a disciplined, non-parametric approach to predicting economic outcomes. \textcite{Mullainathan2017MachineApproach} illustrate how regression trees can enhance predictions of house prices, summarising their findings by asserting that, ``machine learning offers a potent tool to discern, with unprecedented clarity, the messages conveyed by the data.” Thus, ML can serve as a valuable adjunct to traditional model-based methods. \textcite{Chakraborty2017MachineBanks} evaluate the performance of ten econometric and ML models in predicting inflation in the United Kingdom post-Global Financial Crisis, identifying random forests as the superior model in their test samples. Subsequently, \textcite{Medeiros2021ForecastingMethods} assess various ML techniques for forecasting inflation in the United States, also highlighting the superiority of random forests over competing methods. These results echo broader findings by \textcite{Fernandez-Delgado2014DoProblems}, who evaluated 179 classifier models across 121 datasets and found random forests to excel as the top performer. Furthermore, \textcite{GouletCoulombe2024TheForest} makes an initial effort to integrate random forest methodology with a macroeconomic model. 

The increasing focus on random forests informs the focus of this paper \autocite{Kohlscheen2021WhatInflation}. Random forest models nonlinear nonparametric models that have attracting an increasingly large audience in economics \autocite{Athey2019TheEconomics}.

\subsection{Paper Overview}

The selection of the models in this paper reflects the realities faced by macroeconomic forecasters. If an omniscient planner had access to both perfect data and boundless computational prowess, the approach to forecasting inflation would transcend traditional methodologies. Such a planner would have access to non-traditional data like real-time biometric economic indicators from individuals reflecting their spending propensity and stress levels linked to financial stability, detailed satellite imagery tracking raw material movements globally, and predictive logistics data foreseeing supply chain disruptions before they occur. Futuristic data like genetic algorithms predicting entrepreneurial success and the economic impact of unpatented innovations could also feed into this vast informational nexus.

Leveraging infinite computational power, the planner would integrate these vast datasets into a hybrid model combining traditional econometric methods with cutting-edge machine learning techniques. Structural economic models would be used to ensure the incorporation of well-established economic theories, while deep learning networks would manage the sheer scale and complexity of the data, capturing subtle nonlinearities and interactions that traditional models might miss. This model would not only continuously update in real-time as new data becomes available, but also simulate countless economic scenarios to predict the impacts of potential economic shocks or policy interventions. This ultimate model would serve as more than a forecasting tool; it would be an economic oracle, capable of advising policymakers on the optimal interventions to avoid inflationary pressures or deflationary spirals before they materialize. 

In the absence of such a system, forecasting needs to make use of macroeconomic data as it exists today. That is to say, data that is often untimely and subject to substantial revision. While there are increasingly novel datasets, like satellite imagery, most macroeconomists, especially those with significantly policymaking responsibility, have not seen fit to integrate them into their forecasting systems as yet. There are, however, as this paper will now explore, an increasingly powerful array of machine learning tools (and improving time series macro data) that are finally allowing us to improve the accuracy of macroeconomic forecasting.

Having situated the paper in the literature on inflation forecasting in the section above, \S \ref{sec:lit_review}, I now move to the substantive section of the paper. The goal of this paper is to compare methods for forecasting US inflation. Specifically, I employ a series of models to forecast inflation one month ahead, specifically, the month-over-month log change in US CPI. In \S \ref{sec:methods_models} I formally introduce the models estimated in the paper, an autoregressive (AR) model, a Random Forest (RF) and Least Absolute Shrinkage and Selection Operator (Lasso) model. I then discuss the data used in the paper, the FRED-MD macroeconomic dataset, in \S \ref{sec:methods_data}. In \S \ref{sec:analysis}, I then present and analyse the results from the models. 
